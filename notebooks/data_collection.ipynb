{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/manzi/.anaconda3/lib/python3.8/site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/manzi/.anaconda3/lib/python3.8/site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manzi/.anaconda3/lib/python3.8/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/manzi/.anaconda3/lib/python3.8/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/manzi/.anaconda3/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyDlrOVHZ8stkQCS52a1qf2f06zrvwnVPGw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://youtube.googleapis.com/youtube/v3/\"\n",
    "headers = {'Accept': 'application/json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_from_file(directory, filename):\n",
    "    with open(os.path.join(directory, filename)) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_file(directory, data, filename=None):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    if (filename is None):\n",
    "        filename = '{0}.json'.format(time.time())\n",
    "\n",
    "    with open(os.path.join(directory, filename), 'w') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video_ids_to_url_file(directory, video_ids):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    filename = '{0}.csv'.format(time.time())\n",
    "    \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in video_ids:\n",
    "            f.write(\"http://www.youtube.com/watch?v={0}\".format(item))\n",
    "            f.write(\"\\r\\n\")\n",
    "            \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(keyword, next_page_token):\n",
    "    part = \"snippet\"\n",
    "    max_results = 50\n",
    "    res_type = \"video\"\n",
    "    \n",
    "    url = '{0}search?part={1}&maxResults={2}&type={3}&q={4}&key={5}'.format(\n",
    "        base_url, \n",
    "        part,\n",
    "        max_results, \n",
    "        res_type,\n",
    "        keyword, \n",
    "        API_KEY\n",
    "    )\n",
    "    \n",
    "    if (next_page_token is not None):\n",
    "        url += '&pageToken={0}'.format(next_page_token)\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content.decode('utf-8'))\n",
    "    else:\n",
    "        raise Exception(\"HTTP not OK: {0}\".format(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids(search_data):\n",
    "    \n",
    "#     Initialize list\n",
    "    video_ids = set()\n",
    "\n",
    "#     Parse JSON\n",
    "    for search_result in search_data[\"items\"]:\n",
    "        video_ids.add(search_result[\"id\"][\"videoId\"]);\n",
    "\n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(video_ids):\n",
    "    \n",
    "    csv_video_ids = \",\".join(video_ids);\n",
    "    \n",
    "    part = \"snippet,contentDetails\"\n",
    "    url = \"{0}videos?part={1}&id={2}&key={3}\".format(\n",
    "        base_url,\n",
    "        part,\n",
    "        csv_video_ids,\n",
    "        API_KEY\n",
    "    )\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content.decode('utf-8'))[\"items\"]\n",
    "    else:\n",
    "        raise Exception(\"HTTP not OK: {0}\".format(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_downloader(url_file_path):\n",
    "    # Invoke external YouTube downloader with YouTube URL file\n",
    "    bashCommand = \"youtube-dlc -a {0} -j > output_video_traces.temp\".format(url_file_path)\n",
    "\n",
    "    print(\"-->Running youtube-dlc: {0}\".format(bashCommand))\n",
    "\n",
    "    !{bashCommand}\n",
    "    \n",
    "    # Open output_video_details file and make a proper array out of it\n",
    "    with open(\"output_video_traces.temp\") as temp_file:\n",
    "        json_strings = temp_file.readlines()\n",
    "\n",
    "        json_array = []\n",
    "\n",
    "        for json_str in json_strings:\n",
    "            json_array.append(json.loads(json_str))\n",
    "\n",
    "        # Save json array to disk\n",
    "        write_json_to_file(\"traces\", json_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_work(search_term, page_limit, video_ids_cache):\n",
    "    \n",
    "    # Count previously found videos\n",
    "    initial_video_ids_cache_size = len(video_ids_cache)\n",
    "    \n",
    "    next_page_token = None\n",
    "    \n",
    "    for i in range(page_limit):\n",
    "        \n",
    "        # Search YouTube API\n",
    "        search_result = search(search_term, next_page_token)\n",
    "        # Save search result\n",
    "        write_json_to_file(\"searches\", search_result)\n",
    "        \n",
    "        # Parse search result to get video ids\n",
    "        found_video_ids = get_video_ids(search_result)\n",
    "        \n",
    "        # Find which found IDs are not duplicates\n",
    "        unique_video_ids = found_video_ids.difference(video_ids_cache)\n",
    "        \n",
    "        # Obtain video details (including projection method)\n",
    "        video_details = get_video_details(unique_video_ids)\n",
    "        # Save video details\n",
    "        #write_json_to_file(\"video_details\", list(unique_video_ids))\n",
    "        \n",
    "        # Find which of our video IDs actually refer to 360 videos\n",
    "        for video_details_item in video_details:\n",
    "            if (video_details_item['contentDetails']['projection'] != \"360\"):\n",
    "                # This is not a 360 video; remove it from the set\n",
    "                unique_video_ids.remove(video_details_item['id'])\n",
    "        \n",
    "        # We now have a local set (unique_video_ids) which contains unique 360 video IDs\n",
    "        \n",
    "        # Check if our unique video ids set is not empty\n",
    "        if (len(unique_video_ids) != 0):\n",
    "            # Save video IDs as URLs to hand to external YouTube Downloader\n",
    "            url_file_path = write_video_ids_to_url_file(\"videos\", unique_video_ids)\n",
    "\n",
    "            # Add found video IDs to our set (no duplicates)\n",
    "            video_ids_cache.update(unique_video_ids)\n",
    "\n",
    "            # Download video traces for the URLs found\n",
    "            print(\"->Downloading [{0}] 360-degree video traces...\".format(len(unique_video_ids)))\n",
    "            youtube_downloader(url_file_path)\n",
    "        \n",
    "        print(\"Processed [{0}/{1}] search pages. Found and stored [{2}] video traces so far.\".format(\n",
    "            i + 1,\n",
    "            page_limit,\n",
    "            len(video_ids_cache) - initial_video_ids_cache_size))\n",
    "        \n",
    "        # Check if there is a next page available\n",
    "        if ('nextPageToken' in search_result):\n",
    "            # Update next_page_token for subsequent searches\n",
    "            next_page_token = search_result['nextPageToken']\n",
    "        elif (i < page_limit - 1):\n",
    "            # There are no more search results for us after this\n",
    "            print(\"No more search results to explore for this search term: \\\"{0}\\\". Exiting...\".format(search_term))\n",
    "            break\n",
    "        \n",
    "    print(\"Processed and stored a total of {0} new 360-degree video traces.\".format(\n",
    "        len(video_ids_cache) - initial_video_ids_cache_size))\n",
    "    \n",
    "    return video_ids_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed [1/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [2/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [3/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [4/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [5/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [6/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [7/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [8/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [9/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [10/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [11/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [12/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [13/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [14/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [15/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [16/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [17/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [18/20] search pages. Found and stored [0] video traces so far.\n",
      "Processed [19/20] search pages. Found and stored [0] video traces so far.\n",
      "Execution interrupted: HTTP not OK: b'{\\n  \"error\": {\\n    \"code\": 403,\\n    \"message\": \"The request cannot be completed because you have exceeded your \\\\u003ca href=\\\\\"/youtube/v3/getting-started#quota\\\\\"\\\\u003equota\\\\u003c/a\\\\u003e.\",\\n    \"errors\": [\\n      {\\n        \"message\": \"The request cannot be completed because you have exceeded your \\\\u003ca href=\\\\\"/youtube/v3/getting-started#quota\\\\\"\\\\u003equota\\\\u003c/a\\\\u003e.\",\\n        \"domain\": \"youtube.quota\",\\n        \"reason\": \"quotaExceeded\"\\n      }\\n    ]\\n  }\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "# Keep track of what (unique) video IDs we have found\n",
    "video_ids_cache = set(read_json_from_file(\"cache\", \"found_video_ids.json\"))\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Execute the program\n",
    "    do_work(\"music\", 20, video_ids_cache)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Execution interrupted: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing found video IDs to cache...\n",
      "Done. Total video traces cached: [245]\n"
     ]
    }
   ],
   "source": [
    "# Store video IDs so that subsequent runs will avoid duplicates\n",
    "print(\"Writing found video IDs to cache...\")\n",
    "write_json_to_file(\"cache\", list(video_ids_cache), \"found_video_ids.json\")\n",
    "\n",
    "print(\"Done. Total video traces cached: [{0}]\".format(len(video_ids_cache)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
